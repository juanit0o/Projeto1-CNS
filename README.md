# CNS repo
Repository with the 3 Computational Numerical Statistics using R.

## Project 1
This Computational Numerical Statistics project was divided into three main groups. These were the generation of random variables, Monte Carlo estimation methods in the domain of integrals as well as confidence intervals.
In the first part we will analyze and study how to generate observations of random variables with known continuous density functions using two methods, the Inverse Transformation Method and the Accept-Reject Method.
Next, for Monte Carlo estimation, we will understand how to estimate the value of a given integral.
In addition to the Naive version, we will use three techniques, Antithetical Variables, Control Variables and Importance Sampling. We will conclude that not all of these techniques have advantages over the original method (the advantage being the percentage of variance reduction) and this will be represented and analysed at the end of the paper.
With regard to confidence intervals, we will discuss, for random samples following a normal distribution, the effect of contaminations on that given sample.

## Project 2
In this second project, we worked on three major topics, as well as some specific variants of certain algorithms.
Firstly, we covered the Jackknife resampling method and did an application exercise using it.
Next, we looked at Bootstrap in order to calculate confidence intervals for unknown parameters as well as estimating the uncertainties associated with them, using Jackknife.
In addition to calculating confidence intervals for a set of data, we also checked whether a postulated hypothesis is likely to be rejected or not, using one of the bootstrap versions.
Subsequently, we looked at the linear regression associated with a set of data that we have for analysis and the estimation associated with the parameters that define that same line also using
Bootstrap, more precisely using two different methodologies, Pairwise Bootstrap and Wild Bootstrap. 
For this type of problem, we carefully verified the following assumptions of normality and linearity to confirm that there is indeed a correlation between the two groups of data.
Finally, in the optimization chapter, we analytically evaluated various estimators for the unknown parameter in the probability density function presented to us and we analyzed the relationship between the likelihood, log-likelihood and score functions.
In this last section, we looked at three other estimation methods, the Bissection Method, Newton-Raphson and Secant, and compared them to the estimates made initially.
